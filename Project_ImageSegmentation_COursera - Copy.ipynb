{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Deep learning using PyTorch : Image segmentation</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/parth1620/Human-Segmentation-Dataset-master.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Human-Segmentation-Dataset-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import cv2\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE='/Human-Segmentation-Dataset-master/train.csv'\n",
    "DATA_DIR='/content/'\n",
    "DEVICE= 'cuda'\n",
    "\n",
    "EPOCHS=25\n",
    "LR=0.003\n",
    "IMG_SIZE=320\n",
    "BATCH_SIZE=16\n",
    "\n",
    "\n",
    "ENCODER='timm-efficientnet-b0'\n",
    "WEIGHTS='imagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(CSV_FILE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row= df.loc[273]     \n",
    "image_path= row.images\n",
    "mask_path=row.masks\n",
    "\n",
    "image= cv2.imread(image_path)\n",
    "image-cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask=cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) /255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "\n",
    "ax1.set_title('IMAGE')\n",
    "ax1.imshow(image)\n",
    "\n",
    "ax2.set_title('MASK')\n",
    "ax2.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df= train_test_split(df, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#The number 42 just corresponds to the seed of Randomness, you can use another number, for example 101. Each time that you run your model, the values selected will be the same, but if you change this number the values selected will be different and your accuracy could have variety but you will be sure that your model is robust if your accuracy keeps following the same. Best regards.\n",
    "#Random_state can be 0 or 1 or any other integer. It should be the same value if you want to validate your processing over multiple runs of the code. By the way, I have seen random_state=42 used in many official examples of scikit.\n",
    "#the random_state parameter is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case.\n",
    "#If random_state is None or np.random, then a randomly-initialized RandomState object is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Functions<br>\n",
    "albumentation documentation : https://albumentations.ai/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "def get_train_augs():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p= 0.5),\n",
    "        A.VerticalFlip(p= 0.5)\n",
    "    ])\n",
    "def get_valid_augs():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, augmentations):\n",
    "        \n",
    "        self.df=df\n",
    "        self.augmentations=augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row= self.df.iloc[idx]\n",
    "\n",
    "        image_path = row.images\n",
    "        mask_path =  row.masks\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # (Height , Width, Channel)\n",
    "        mask = np.expand_dims(mask, axis =-1)\n",
    "\n",
    "        if self.augmentations:\n",
    "            data= self.augmentations(image = image, mask= mask)\n",
    "            image = data['image']\n",
    "            mask = data['mask']\n",
    "\n",
    "        #( Height , Width, Channel) -> (Channel, Height, Width)\n",
    "        image = np.transpose (image, (2,0,1)).astype(np.float32)  #Transposed Image\n",
    "        mask = np.transpose (mask, (2,0,1)).astype(np.float32)    #Transposed Mask\n",
    "\n",
    "        #Convert to numpy \n",
    "        image = torch. Tensor(image)/ 255.0\n",
    "        mask = torch.round(torch.Tensor(mask)/ 255.0)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SegmentationDataset(train_df, get_train_augs())\n",
    "validset= SegmentationDataset(valid_df, get_valid_augs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Trainset  : \", len(trainset))\n",
    "print(\"Size of ValidSet : \", len(validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=12 \n",
    "\n",
    "image, mask = trainset[idx]\n",
    "helper.show_image(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DataSet into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size= BATCH_SIZE, shuffle= True)  # Trainset=232 , batchSize = 16 , so , trainloader= Trainset/ batchsize = 15\n",
    "validloader = DataLoader(validset, batch_size= BATCH_SIZE)  # Validset =58, batchsize =16, so, Validloader = validset/ batchsize = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total no. of batches in trainloader : \",len(trainloader))\n",
    "print(\"Total no. of batches in validloader : \",len(validloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in trainloader:  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One batch image shape : \", image.shape, \" <- Batchsize, No. of channel, IMAGE_SIZE, IMAGE_SIZE\")\n",
    "print(\"One batch mask shape : \", mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Segmentation Model<br>\n",
    "segmentation_models_pytorch documentation : https://smp.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn  \n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "\n",
    "        self.arc = smp.Unet(\n",
    "            encoder_name= ENCODER,\n",
    "            encoder_weights=WEIGHTS,\n",
    "            in_channels= 3,\n",
    "            classes = 1,\n",
    "            activation = None\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, masks =None):\n",
    "        \n",
    "        logits = self.arc(images)\n",
    "\n",
    "        if masks !=None:\n",
    "            loss1= DiceLoss(mode='binary')(logits, masks)   \n",
    "            loss2= nn.BCEWithLogitsLoss()(logits, masks)\n",
    "            return logits, loss1 + loss2\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =SegmentationModel()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train & Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "\n",
    "    for images, masks in tqdm(data_loader): #tqdm is a library in Python which is used for creating Progress Meters or Progress Bars.\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(images, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model):\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader):  \n",
    "            \n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            logits, loss = model(images, masks)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = np.Inf\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    train_loss = train_fn(trainloader, model, optimizer)\n",
    "    valid_loss = eval_fn(validloader, model)\n",
    "\n",
    "if valid_loss < best_valid_loss:   \n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "    print(\"SAVED MODEL\")\n",
    "    best_valid_loss = valid_loss\n",
    "\n",
    "print (\"Epoch : \", i+1,\" Train_loss : \",train_loss,\" Valid_loss : \", valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx= 83\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pt'))  \n",
    "\n",
    "image, mask = validset[idx]\n",
    "\n",
    "logits_mask = model(image.to(DEVICE).unsqueeze(0)) \n",
    "pred_mask = torch.sigmoid(logits_mask)\n",
    "pred_mask = (pred_mask>0.5) *0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper. show_image(image, mask, pred_mask.detach().cpu().squeeze(0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "939b002e18dfaf7d1a3607583eee19df9bce3464fd1a8720950d9657285d0a24"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
